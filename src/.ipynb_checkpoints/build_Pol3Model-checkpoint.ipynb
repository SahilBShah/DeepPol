{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad7cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d382661",
   "metadata": {},
   "source": [
    "## Read in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "231dde16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq = np.load(\"../data/final_data/trainingSequence.npz\")\n",
    "training_dna = seq[\"dna\"].astype(np.int8)\n",
    "training_dna_labels = seq[\"labels\"].astype(np.int8)\n",
    "training_dna_labels = np.where(training_dna_labels == [2], [0], [1])\n",
    "\n",
    "seq = np.load(\"../data/final_data/testingSequence.npz\")\n",
    "testing_dna = seq[\"dna\"].astype(np.int8)\n",
    "testing_dna_labels = seq[\"labels\"].astype(np.int8)\n",
    "testing_dna_labels = np.where(testing_dna_labels == [2], [0], [1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7374fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11162, 2200, 4)\n",
      "(11162, 1)\n",
      "(1222, 2200, 4)\n",
      "(1222, 1)\n"
     ]
    }
   ],
   "source": [
    "print(training_dna.shape)\n",
    "print(training_dna_labels.shape)\n",
    "print(testing_dna.shape)\n",
    "print(testing_dna_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4adf460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_seq = np.load(\"../data/final_data/trainingChip.npz\")\n",
    "training_chip = chip_seq[\"chip\"].astype(np.float32)\n",
    "training_chip_labels = chip_seq[\"labels\"].astype(np.int8)\n",
    "training_chip_labels = np.where(training_chip_labels == [2], [0], [1])\n",
    "\n",
    "chip_seq = np.load(\"../data/final_data/testingChip.npz\")\n",
    "testing_chip = chip_seq[\"chip\"].astype(np.float32)\n",
    "testing_chip_labels = chip_seq[\"labels\"].astype(np.int8)\n",
    "testing_chip_labels = np.where(testing_chip_labels == [2], [0], [1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4fc30bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11162, 2200, 20)\n",
      "(11162, 1)\n",
      "(1222, 2200, 20)\n",
      "(1222, 1)\n"
     ]
    }
   ],
   "source": [
    "print(training_chip.shape)\n",
    "print(training_chip_labels.shape)\n",
    "print(testing_chip.shape)\n",
    "print(testing_chip_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a755c5e2",
   "metadata": {},
   "source": [
    "## Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c0a5271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pol3_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Define your layers here.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=2200,out_channels=320,kernel_size=4, padding=70)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=4, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(320)\n",
    "        self.conv2 = nn.Conv1d(in_channels=320, out_channels=480, kernel_size=4)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=4, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(480)\n",
    "        self.output_layer = nn.Linear(4320, 1)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "        Design the process of your network.\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.conv1(X)\n",
    "        X = self.bn1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.maxpool1(X)\n",
    "        \n",
    "        X = self.conv2(X)\n",
    "        X = self.bn2(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.maxpool2(X)\n",
    "        \n",
    "        X = torch.flatten(X, 1)        \n",
    "\n",
    "        logits = self.output_layer(X)\n",
    "        return logits   # do not apply softmax\n",
    "\n",
    "    def classify(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Write a function that outputs the labels.\n",
    "        \"\"\"\n",
    "        logits = self(X)\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        logits = nn.functional.normalize(logits, dim = 1)\n",
    "        labels = torch.argmax(logits, dim=1)\n",
    "        return labels.type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ce2f2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_data, training_labels, testing_data, testing_labels, epochs=15, batch_size=16, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Q:  write the training loop following the schema shown above.\n",
    "\n",
    "    Inputs\n",
    "    - model: the model to be trained - a PyTorch nn.Module class object\n",
    "    - X_train, y_train, X_val, y_val: training and validation data\n",
    "    - epochs: num epochs, or the number of times we want to run through the entire training data\n",
    "    - batch_size: number of data points per batch\n",
    "    - lr: learning rate\n",
    "    - optimizer: optimizer used\n",
    "\n",
    "    Outputs\n",
    "    - losses: a list of losses\n",
    "    - accuracies: a list of accuracies\n",
    "    \"\"\"\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    best_acc = -1\n",
    "    best_model = None\n",
    "\n",
    "    batches = int(np.ceil(len(training_data) / batch_size))\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    testing_data = torch.tensor(testing_data).type(torch.float32)\n",
    "    testing_labels = torch.tensor(testing_labels).type(torch.float32)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(batches):\n",
    "#             if i % 1000 == 0:\n",
    "#               print(\"Epoch \" + str(epoch+1) + \"/\" + str(epochs) + \": \" + str(i/batches*100) + \"%\")\n",
    "            X_batch = training_data[i*batch_size:i*batch_size+batch_size]\n",
    "            X_batch = torch.tensor(X_batch).type(torch.float32)\n",
    "            y_batch = training_labels[i*batch_size:i*batch_size+batch_size]\n",
    "            y_batch = torch.tensor(y_batch).type(torch.float32)\n",
    "            \n",
    "            logits = model(X_batch)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # calculate the validation accuracy and append the loss of this epoch\n",
    "        y_pred_test = model.classify(testing_data)\n",
    "        accuracy = accuracy_score(testing_labels.cpu(), y_pred_test.cpu())\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "          best_acc = accuracy\n",
    "          best_model = deepcopy(model)\n",
    "\n",
    "        # print epoch, loss, and current test accuracy (don't delete this line - it's slightly more organized now)\n",
    "        print(f\"Epoch {epoch + 1}:\\tloss {np.round(loss.detach().cpu().numpy().item(), 4)}\\t& accuracy {np.round(accuracy, 4)}\")\n",
    "    print(f\"Resetting model... Best validation accuracy:\\t{np.round(best_acc, 4)}\")\n",
    "    model.load_state_dict(best_model.state_dict())\n",
    "    return losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3da51bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tloss -0.0\t& accuracy 0.5\n",
      "Epoch 2:\tloss -0.0\t& accuracy 0.5\n",
      "Epoch 3:\tloss -0.0\t& accuracy 0.5\n",
      "Epoch 4:\tloss -0.0\t& accuracy 0.5\n",
      "Epoch 5:\tloss -0.0\t& accuracy 0.5\n",
      "Epoch 6:\tloss -0.0\t& accuracy 0.5\n",
      "Epoch 7:\tloss -0.0\t& accuracy 0.5\n",
      "Epoch 8:\tloss -0.0\t& accuracy 0.5\n",
      "Epoch 9:\tloss -0.0\t& accuracy 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [113]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Pol3_CNN()\n\u001b[0;32m----> 2\u001b[0m losses, accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dna_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_dna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_dna_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [112]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, training_data, training_labels, testing_data, testing_labels, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, y_batch)\n\u001b[1;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# calculate the validation accuracy and append the loss of this epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Pol3_CNN()\n",
    "losses, accuracies = train(model, training_dna, training_dna_labels, testing_dna, testing_dna_labels, batch_size=100, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fe20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
